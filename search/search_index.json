{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home Page WELCOME YOU ARE I welcome you to my Embedded Linux blog which contains my experience with it and some tutorials about software and hardware skills that I have picked up along the way. Introduction This blog can be summed up in a line: Linux is a cruel mistress and Embedded Linux is that bitch after a break up If you understand the above quote then let's start digging up some Linux Gold , and if you have not then this is not the right place for you please do yourself a favour and readup on Linux and it's relatives and come back with a stronger will . Need of this blog There are a lot of places where tech noobs spread fake information and facts. Herein I try to be as specific as I can about the stuff I write so I don't make the noob mistakes. All the information that I present here is totally according to my experience since I have done all of them individually I can only vouch for their applicability to a limited extent and would be happy to help in case of difficulties.","title":"Home"},{"location":"#home-page","text":"WELCOME YOU ARE I welcome you to my Embedded Linux blog which contains my experience with it and some tutorials about software and hardware skills that I have picked up along the way.","title":"Home Page"},{"location":"#introduction","text":"This blog can be summed up in a line: Linux is a cruel mistress and Embedded Linux is that bitch after a break up If you understand the above quote then let's start digging up some Linux Gold , and if you have not then this is not the right place for you please do yourself a favour and readup on Linux and it's relatives and come back with a stronger will .","title":"Introduction"},{"location":"#need-of-this-blog","text":"There are a lot of places where tech noobs spread fake information and facts. Herein I try to be as specific as I can about the stuff I write so I don't make the noob mistakes. All the information that I present here is totally according to my experience since I have done all of them individually I can only vouch for their applicability to a limited extent and would be happy to help in case of difficulties.","title":"Need of this blog"},{"location":"About/about/","text":"About Prathu Baronia ( alias: praton ) Current Position Fifth Year Undergraduate Embedded Linux Researcher at AJIT Research Group IIT Bombay, India Past Work Experiences Embedded System Engineer at Greetude Energy Systems (May'14 - Jul'14) Other Blogs My Blogspot blog on some of my epiphanies Time Contact Info StackOverflow Github IITB Homepage Personal email My Resume","title":"About"},{"location":"About/about/#about","text":"Prathu Baronia ( alias: praton )","title":"About"},{"location":"About/about/#current-position","text":"Fifth Year Undergraduate Embedded Linux Researcher at AJIT Research Group IIT Bombay, India","title":"Current Position"},{"location":"About/about/#past-work-experiences","text":"Embedded System Engineer at Greetude Energy Systems (May'14 - Jul'14)","title":"Past Work Experiences"},{"location":"About/about/#other-blogs","text":"My Blogspot blog on some of my epiphanies Time","title":"Other Blogs"},{"location":"About/about/#contact-info","text":"StackOverflow Github IITB Homepage Personal email My Resume","title":"Contact Info"},{"location":"Guides/How_to_Embedded_Resume/main/","text":"A Guide to making an Embedded Resume Work Experience Projects Techincal Skills","title":"How to make an Embedded Resume"},{"location":"Guides/How_to_Embedded_Resume/main/#a-guide-to-making-an-embedded-resume","text":"","title":"A Guide to making an Embedded Resume"},{"location":"Guides/How_to_Embedded_Resume/main/#work-experience","text":"","title":"Work Experience"},{"location":"Guides/How_to_Embedded_Resume/main/#projects","text":"","title":"Projects"},{"location":"Guides/How_to_Embedded_Resume/main/#techincal-skills","text":"","title":"Techincal Skills"},{"location":"Guides/How_to_use_mutt/main/","text":"","title":"How to use mutt"},{"location":"OS/BIOS_UEFI/","text":"BIOS The Basic Input/Output System ( BIOS ) is a firmware interface that controls not only the first step of the boot process, but also provides the lowest level interface to peripheral devices. Boot Process When the system boots, the processor looks at the end of system memory for the BIOS program , and runs it. BIOS onducts POST (Power On Self Test). Fetches MBR (Master Boot Record) from first sector of memory. MBR is 512 bytes in size MBR contains machine code instructions for booting the machine, called a boot loader, along with the partition table. BIOS loads Bootloader in RAM , the control is passed to Bootloader after the BIOS loads it. First-stage bootloader is a small machine code binary on the MBR. Its sole job is to locate the second stage boot loader (GRUB) and load the first part of it into memory. Limitations BIOS can boot from only drives of less than or equal to 2 TB . It can't initialize multiple hardware devices at once leading to slow boot processes. Can't support advanced features like netboot . UEFI The Unified Extensible Firmware Interface (UEFI) is designed, like BIOS, to control the boot process (through boot services) and to provide an interface between system firmware and an operating system (through runtime services). Unlike BIOS, it features its own architecture, independent of the CPU, and its own device drivers. UEFI can mount partitions and read certain file systems. Boot Process When an x86 computer equipped with UEFI boots, the interface searches the system storage for a partition labeled with a specific globally unique identifier (GUID) that marks it as the EFI System Partition (ESP) . This partition contains applications compiled for the EFI architecture , which might include bootloaders for operating systems and utility software. UEFI systems include an EFI boot manager that can boot the system from a default configuration, or prompt a user to choose an operating system to boot. When a bootloader is selected, manually or automatically, UEFI reads it into memory and yields control of the boot process to it. If everything works out fine it boots but if it fails it falls back to BIOS type booting which is known as the Legacy Boot Mode . Advantages Over BIOS No limitation over size anytime soon due to 64-bit entries in GPT . UI is better (Needs to be explained a better) Provides Secure Boot feature which has been discussed below. Secure Boot","title":"BIOS & UEFI"},{"location":"OS/BIOS_UEFI/#bios","text":"The Basic Input/Output System ( BIOS ) is a firmware interface that controls not only the first step of the boot process, but also provides the lowest level interface to peripheral devices.","title":"BIOS"},{"location":"OS/BIOS_UEFI/#boot-process","text":"When the system boots, the processor looks at the end of system memory for the BIOS program , and runs it. BIOS onducts POST (Power On Self Test). Fetches MBR (Master Boot Record) from first sector of memory. MBR is 512 bytes in size MBR contains machine code instructions for booting the machine, called a boot loader, along with the partition table. BIOS loads Bootloader in RAM , the control is passed to Bootloader after the BIOS loads it. First-stage bootloader is a small machine code binary on the MBR. Its sole job is to locate the second stage boot loader (GRUB) and load the first part of it into memory.","title":"Boot Process"},{"location":"OS/BIOS_UEFI/#limitations","text":"BIOS can boot from only drives of less than or equal to 2 TB . It can't initialize multiple hardware devices at once leading to slow boot processes. Can't support advanced features like netboot .","title":"Limitations"},{"location":"OS/BIOS_UEFI/#uefi","text":"The Unified Extensible Firmware Interface (UEFI) is designed, like BIOS, to control the boot process (through boot services) and to provide an interface between system firmware and an operating system (through runtime services). Unlike BIOS, it features its own architecture, independent of the CPU, and its own device drivers. UEFI can mount partitions and read certain file systems.","title":"UEFI"},{"location":"OS/BIOS_UEFI/#boot-process_1","text":"When an x86 computer equipped with UEFI boots, the interface searches the system storage for a partition labeled with a specific globally unique identifier (GUID) that marks it as the EFI System Partition (ESP) . This partition contains applications compiled for the EFI architecture , which might include bootloaders for operating systems and utility software. UEFI systems include an EFI boot manager that can boot the system from a default configuration, or prompt a user to choose an operating system to boot. When a bootloader is selected, manually or automatically, UEFI reads it into memory and yields control of the boot process to it. If everything works out fine it boots but if it fails it falls back to BIOS type booting which is known as the Legacy Boot Mode .","title":"Boot Process"},{"location":"OS/BIOS_UEFI/#advantages-over-bios","text":"No limitation over size anytime soon due to 64-bit entries in GPT . UI is better (Needs to be explained a better) Provides Secure Boot feature which has been discussed below.","title":"Advantages Over BIOS"},{"location":"OS/BIOS_UEFI/#secure-boot","text":"","title":"Secure Boot"},{"location":"OS/vmap/","text":"Virtual Mapping of Memory","title":"Virtual Mapping"},{"location":"OS/vmap/#virtual-mapping-of-memory","text":"","title":"Virtual Mapping of Memory"},{"location":"Processor/basics/","text":"Basics of Processor design","title":"Basics"},{"location":"Processor/basics/#basics-of-processor-design","text":"","title":"Basics of Processor design"},{"location":"Processor/pipelined/","text":"","title":"Pipelined"},{"location":"Processor/reservation_station/","text":"Design of a Reservation Station","title":"Reservation Station"},{"location":"Processor/reservation_station/#design-of-a-reservation-station","text":"","title":"Design of a Reservation Station"},{"location":"Processor/superscalar/","text":"","title":"Superscalar"},{"location":"Projects/Archived/Processor_Link/","text":"Processor Link Need for the Processor Link For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations. Driver API The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... } Approach to write the driver Driver infrastructure can be broken down in four simple parts: fpga_open(...) As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions. fpga_close(...) As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device. send_u64_to_processor(...) This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns. recv_u64_from_processor(...) This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one. Explaining polling method The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made. Need for interrupt based support in future To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus. Testing of the driver Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"Processor Link"},{"location":"Projects/Archived/Processor_Link/#processor-link","text":"","title":"Processor Link"},{"location":"Projects/Archived/Processor_Link/#need-for-the-processor-link","text":"For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations.","title":"Need for the Processor Link"},{"location":"Projects/Archived/Processor_Link/#driver-api","text":"The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... }","title":"Driver API"},{"location":"Projects/Archived/Processor_Link/#approach-to-write-the-driver","text":"Driver infrastructure can be broken down in four simple parts:","title":"Approach to write the driver"},{"location":"Projects/Archived/Processor_Link/#fpga_open","text":"As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions.","title":"fpga_open(...)"},{"location":"Projects/Archived/Processor_Link/#fpga_close","text":"As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device.","title":"fpga_close(...)"},{"location":"Projects/Archived/Processor_Link/#send_u64_to_processor","text":"This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns.","title":"send_u64_to_processor(...)"},{"location":"Projects/Archived/Processor_Link/#recv_u64_from_processor","text":"This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one.","title":"recv_u64_from_processor(...)"},{"location":"Projects/Archived/Processor_Link/#explaining-polling-method","text":"The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made.","title":"Explaining polling method"},{"location":"Projects/Archived/Processor_Link/#need-for-interrupt-based-support-in-future","text":"To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus.","title":"Need for interrupt based support in future"},{"location":"Projects/Archived/Processor_Link/#testing-of-the-driver","text":"Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"Testing of the driver"},{"location":"Projects/Archived/pcie_driver/","text":"Design of a C based PCIe driver Need for the Processor Link For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations. Driver API The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... } Approach to write the driver Driver infrastructure can be broken down in four simple parts: fpga_open(...) As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions. fpga_close(...) As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device. send_u64_to_processor(...) This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns. recv_u64_from_processor(...) This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one. Explaining polling method The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made. Need for interrupt based support in future To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus. Testing of the driver Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"PCIe C driver"},{"location":"Projects/Archived/pcie_driver/#design-of-a-c-based-pcie-driver","text":"","title":"Design of a C based PCIe driver"},{"location":"Projects/Archived/pcie_driver/#need-for-the-processor-link","text":"For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations.","title":"Need for the Processor Link"},{"location":"Projects/Archived/pcie_driver/#driver-api","text":"The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... }","title":"Driver API"},{"location":"Projects/Archived/pcie_driver/#approach-to-write-the-driver","text":"Driver infrastructure can be broken down in four simple parts:","title":"Approach to write the driver"},{"location":"Projects/Archived/pcie_driver/#fpga_open","text":"As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions.","title":"fpga_open(...)"},{"location":"Projects/Archived/pcie_driver/#fpga_close","text":"As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device.","title":"fpga_close(...)"},{"location":"Projects/Archived/pcie_driver/#send_u64_to_processor","text":"This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns.","title":"send_u64_to_processor(...)"},{"location":"Projects/Archived/pcie_driver/#recv_u64_from_processor","text":"This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one.","title":"recv_u64_from_processor(...)"},{"location":"Projects/Archived/pcie_driver/#explaining-polling-method","text":"The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made.","title":"Explaining polling method"},{"location":"Projects/Archived/pcie_driver/#need-for-interrupt-based-support-in-future","text":"To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus.","title":"Need for interrupt based support in future"},{"location":"Projects/Archived/pcie_driver/#testing-of-the-driver","text":"Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"Testing of the driver"},{"location":"Projects/Current/ajit_hpc/","text":"FPGA HPC System around AJIT Introduction AJIT is India's first indigenous processor which has been designed at IIT Bombay and is currently in its second design and manufacturing iteration. AJIT is based on the open SPARC-v8 ISA and is currently a 32-bit processor. Post processor design and testing phase we head out to develop a system around the processor in order to create necessary peripheral interfaces on the VC709 FPGA. The plan is to create an independent system which would have running an Operating system designed specifically for AJIT on it, this process is well known as creating a port of an OS to AJIT. Here in this report we describea general peripheral interface plan and the necessary peripherals required to boot an OS and how one can go about creating them and interfacing them with AJIT. Necessary Goals to achieve The necessary tasks that we need to accomplish in order to run an OS on AJIT are: Fulfilling the processor's need of larger DRAM to boot the OS Providing direct access to the debug interface to the processor in the new interface plan Interface of Ajit's internal Ajit FIFO Bus (AFB) to the generic AXI-Lite A host side driver which provides read/write access and is part of the debug interface And finally an OS ported to AJIT Roadmap of the project till now Tasks Timeline DRAM Interface development Jul'18 PCIe-AXI Interface development Jul'18 - Aug'18 HLS synthesis of Debug FIFOs and their controllers Aug'18 Custom IP packaging of inhouse AFB-AXI Interface Aug'18-Sep'18 Development of a Processor Link Interface Sep'18 OS Porting to AJIT Sep'18 - Oct'18 Current Stage of the project Testing of FPGA model for memory speeds offered which directly affects data access time and more critically boot time. Testing of processor link interface for intermediate boot critical debugging and serial console printing. Setting up of past contributions in the OS port and looking forward in the directions to improve the boot-debug cycle. Collaborators Mr. Datar Mandar, Phd, HPC Lab, IIT Bombay Abbreviations Used Abbreviation Full Form PCIe Peripheral Component Interconnect Express AFB AJIT FIFO Bus WIP Work in progress TIP Testing in progress Designing the HPC System","title":"AJIT HPC System"},{"location":"Projects/Current/ajit_hpc/#fpga-hpc-system-around-ajit","text":"","title":"FPGA HPC System around AJIT"},{"location":"Projects/Current/ajit_hpc/#introduction","text":"AJIT is India's first indigenous processor which has been designed at IIT Bombay and is currently in its second design and manufacturing iteration. AJIT is based on the open SPARC-v8 ISA and is currently a 32-bit processor. Post processor design and testing phase we head out to develop a system around the processor in order to create necessary peripheral interfaces on the VC709 FPGA. The plan is to create an independent system which would have running an Operating system designed specifically for AJIT on it, this process is well known as creating a port of an OS to AJIT. Here in this report we describea general peripheral interface plan and the necessary peripherals required to boot an OS and how one can go about creating them and interfacing them with AJIT.","title":"Introduction"},{"location":"Projects/Current/ajit_hpc/#necessary-goals-to-achieve","text":"The necessary tasks that we need to accomplish in order to run an OS on AJIT are: Fulfilling the processor's need of larger DRAM to boot the OS Providing direct access to the debug interface to the processor in the new interface plan Interface of Ajit's internal Ajit FIFO Bus (AFB) to the generic AXI-Lite A host side driver which provides read/write access and is part of the debug interface And finally an OS ported to AJIT","title":"Necessary Goals to achieve"},{"location":"Projects/Current/ajit_hpc/#roadmap-of-the-project-till-now","text":"Tasks Timeline DRAM Interface development Jul'18 PCIe-AXI Interface development Jul'18 - Aug'18 HLS synthesis of Debug FIFOs and their controllers Aug'18 Custom IP packaging of inhouse AFB-AXI Interface Aug'18-Sep'18 Development of a Processor Link Interface Sep'18 OS Porting to AJIT Sep'18 - Oct'18","title":"Roadmap of the project till now"},{"location":"Projects/Current/ajit_hpc/#current-stage-of-the-project","text":"Testing of FPGA model for memory speeds offered which directly affects data access time and more critically boot time. Testing of processor link interface for intermediate boot critical debugging and serial console printing. Setting up of past contributions in the OS port and looking forward in the directions to improve the boot-debug cycle.","title":"Current Stage of the project"},{"location":"Projects/Current/ajit_hpc/#collaborators","text":"Mr. Datar Mandar, Phd, HPC Lab, IIT Bombay","title":"Collaborators"},{"location":"Projects/Current/ajit_hpc/#abbreviations-used","text":"Abbreviation Full Form PCIe Peripheral Component Interconnect Express AFB AJIT FIFO Bus WIP Work in progress TIP Testing in progress","title":"Abbreviations Used"},{"location":"Projects/Current/ajit_hpc/#designing-the-hpc-system","text":"","title":"Designing the HPC System"},{"location":"Projects/Current/iitb_risc_superscalar/","text":"IITB-RISC Superscalar Design","title":"IITB-RISC Superscalar design"},{"location":"Projects/Current/iitb_risc_superscalar/#iitb-risc-superscalar-design","text":"","title":"IITB-RISC Superscalar Design"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/","text":"AFB-AXI Interface Custom IP Generation Need for a Custom IP for AFB-AXI Interface AFB (Ajit FIFO Bus) is an inhouse term coined to represent the data bus of AJIT which allows it to talk to other AXI peripherals. AFB is not directly AXI compatible and thus needs a bridge, we developed it from scratch in Verilog keeping in mind the provided AFB specifications. AXI-Lite Description This interface operates with 5 FIFOs with different widths namely for Write Address, Write Data, Write Response, Read Address and Read data channel . Our implementation We generated a custom IP bridge by writing a verilog file and then packaging an IP over it to make it available in the User repository in Vivado. The bridge supports AXI-Lite on one side and AFB on the other. Full Flow The figure below shows the full flow model integrated with the AFB-AXI bridge. Testing of Interface We conducted manual memory read-write tests on the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"AFB-AXI Interface Custom IP Generation"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#afb-axi-interface-custom-ip-generation","text":"","title":"AFB-AXI Interface Custom IP Generation"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#need-for-a-custom-ip-for-afb-axi-interface","text":"AFB (Ajit FIFO Bus) is an inhouse term coined to represent the data bus of AJIT which allows it to talk to other AXI peripherals. AFB is not directly AXI compatible and thus needs a bridge, we developed it from scratch in Verilog keeping in mind the provided AFB specifications.","title":"Need for a Custom IP for AFB-AXI Interface"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#axi-lite-description","text":"This interface operates with 5 FIFOs with different widths namely for Write Address, Write Data, Write Response, Read Address and Read data channel .","title":"AXI-Lite Description"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#our-implementation","text":"We generated a custom IP bridge by writing a verilog file and then packaging an IP over it to make it available in the User repository in Vivado. The bridge supports AXI-Lite on one side and AFB on the other.","title":"Our implementation"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#full-flow","text":"The figure below shows the full flow model integrated with the AFB-AXI bridge.","title":"Full Flow"},{"location":"Projects/Current/ajit_hpc_blocks/AFB_AXI_Interface/#testing-of-interface","text":"We conducted manual memory read-write tests on the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"Testing of Interface"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/","text":"DRAM Interface Need for a DRAM interface In the previous iteration of OS booting on AJIT processor the OS utilized the block RAM of the processor which was of size 4 MB and thus took awhile to boot and only a small scale OS without much driver support(like Network drivers) could be booted, thus a need for a larger DRAM support. Here we head out to utilize the onboard DRAM of the VC709 board based on the ideas of our generic peripheral interface discussed before. Our implementation The figure below shows our DRAM interface implementation as part of a generic AXI peripheral system. DRAM interfaces with the AXI interconnect with the aidof a Memory Controller specifically employed for DRAMs. Memory controller was generated using the MIG (Memory Interface Generator) utility of Vivado 2017.1. The support for DRAM size here goes up to 4 GB per slot and since VC79 has two slots we can have a combined support upto 8GB RAM. MIG Vs EMC Core The two cores provided in Vivado are for different types of memory. The following information can be found in the repsective user guides which summarise their usecases: Core Memory Support MIG DDR2, DDR3, QDR-II, RLDRAM EMC SRAM, ZBT, and other similar SRAM like interfaces Testing of Interface We tested the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"DRAM Interface"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/#dram-interface","text":"","title":"DRAM Interface"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/#need-for-a-dram-interface","text":"In the previous iteration of OS booting on AJIT processor the OS utilized the block RAM of the processor which was of size 4 MB and thus took awhile to boot and only a small scale OS without much driver support(like Network drivers) could be booted, thus a need for a larger DRAM support. Here we head out to utilize the onboard DRAM of the VC709 board based on the ideas of our generic peripheral interface discussed before.","title":"Need for a DRAM interface"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/#our-implementation","text":"The figure below shows our DRAM interface implementation as part of a generic AXI peripheral system. DRAM interfaces with the AXI interconnect with the aidof a Memory Controller specifically employed for DRAMs. Memory controller was generated using the MIG (Memory Interface Generator) utility of Vivado 2017.1. The support for DRAM size here goes up to 4 GB per slot and since VC79 has two slots we can have a combined support upto 8GB RAM.","title":"Our implementation"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/#mig-vs-emc-core","text":"The two cores provided in Vivado are for different types of memory. The following information can be found in the repsective user guides which summarise their usecases: Core Memory Support MIG DDR2, DDR3, QDR-II, RLDRAM EMC SRAM, ZBT, and other similar SRAM like interfaces","title":"MIG Vs EMC Core"},{"location":"Projects/Current/ajit_hpc_blocks/DRAM_without_PCIe/#testing-of-interface","text":"We tested the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"Testing of Interface"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/","text":"FIFO Controller Generation Need for a Memory Mapped FIFO The Debug interface to the processor requires two 64-bit FIFOs compatible with the generic AXI periheral interface. Here by compatibility we mean they need to have a Slave AXI interface in order to establish a bidirectional data flow thorugh the PCIe-AXI interface. The problem is solved by dividing it into two parts and both of them handling simpler generic tasks. The first block provides the Slave AXI interface as an input and a control interface on the output side to the other block which acts as a data FIFO. A similar block is created to perform the reverse operation as well. Our implementation HLS Generation We employ High Level Synthesis to generate the FIFO controllers. HLS enables us to write short code snippets as shown below to generate the Slave AXI interfaces and control signals. { Explain the use of pragmas } void fifo_to_axi_slave (ap_uint 32 *data_out, bool *data_valid, hls::stream ap_uint 32 in_fifo) { #pragma HLS INTERFACE s_axilite port=return #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=data_valid if (!in_fifo.empty ()) { *data_out = in_fifo.read (); *data_valid = true; } else { *data_valid = false; } } Full Flow The figure below shows our DRAM interface implementation as part of a generic AXI peripheral system. DRAM interfaces with the AXI interconnect with the aidof a Memory Controller specifically employed for DRAMs. Memory controller was generated using the MIG (Memory Interface Generator) utility of Vivado 2017.1. Testing of Interface We tested the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"FIFO Controller Generation"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#fifo-controller-generation","text":"","title":"FIFO Controller Generation"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#need-for-a-memory-mapped-fifo","text":"The Debug interface to the processor requires two 64-bit FIFOs compatible with the generic AXI periheral interface. Here by compatibility we mean they need to have a Slave AXI interface in order to establish a bidirectional data flow thorugh the PCIe-AXI interface. The problem is solved by dividing it into two parts and both of them handling simpler generic tasks. The first block provides the Slave AXI interface as an input and a control interface on the output side to the other block which acts as a data FIFO. A similar block is created to perform the reverse operation as well.","title":"Need for a Memory Mapped FIFO"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#our-implementation","text":"","title":"Our implementation"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#hls-generation","text":"We employ High Level Synthesis to generate the FIFO controllers. HLS enables us to write short code snippets as shown below to generate the Slave AXI interfaces and control signals. { Explain the use of pragmas } void fifo_to_axi_slave (ap_uint 32 *data_out, bool *data_valid, hls::stream ap_uint 32 in_fifo) { #pragma HLS INTERFACE s_axilite port=return #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=data_valid if (!in_fifo.empty ()) { *data_out = in_fifo.read (); *data_valid = true; } else { *data_valid = false; } }","title":"HLS Generation"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#full-flow","text":"The figure below shows our DRAM interface implementation as part of a generic AXI peripheral system. DRAM interfaces with the AXI interconnect with the aidof a Memory Controller specifically employed for DRAMs. Memory controller was generated using the MIG (Memory Interface Generator) utility of Vivado 2017.1.","title":"Full Flow"},{"location":"Projects/Current/ajit_hpc_blocks/FIFO_Controller_Generation/#testing-of-interface","text":"We tested the interface using the XSCT command line utility which employs the JTAG interface and gives a direct access to all AXI peripherals. Later on we also tested this with a PCI express inerface from the Host CPU using the pcimem utility.","title":"Testing of Interface"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/","text":"Here we introduce a genercic interface architecture for building a system. As shown in the figure above the generic interface diagram of the fpga model. Every subsystem including the processor core hanging from the interconnect appear just like a memory mapped peripheral to it. This kind of architecture as explained at last in future work would provide a relatively easy extension to a Multiprocessor SMP model as compared to a processor centric model. Some periherals would act as AXI Slaves and some as AXI Masters depending on their functionality. Another advantage to this architecture is that since each peripheral is memory mapped and JTAG has a direct access to the Interconnect one can probe and read-write from-to any peripheral independent of the processor. As shown later after interfacing the PCIe to AXI we get another possible data flow channel other than the standard UART and JTAG. The number of peripherals that can be interfaced in this fashion is only limited by the number of bits used for addressing on the interconnect. DRAM Memory Interfacing PCIe-AXI Bridge Debug Bridge AFB-AXI Bridge AJIT Core Combining the IPs Interface creation in IP Blocks Additional IPs Memory Address Translator C driver for PCIe interfacing Software Testbench","title":"Generic interface"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#dram-memory-interfacing","text":"","title":"DRAM Memory Interfacing"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#pcie-axi-bridge","text":"","title":"PCIe-AXI Bridge"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#debug-bridge","text":"","title":"Debug Bridge"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#afb-axi-bridge","text":"","title":"AFB-AXI Bridge"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#ajit-core","text":"","title":"AJIT Core"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#combining-the-ips","text":"","title":"Combining the IPs"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#interface-creation-in-ip-blocks","text":"","title":"Interface creation in IP Blocks"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#additional-ips","text":"","title":"Additional IPs"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#memory-address-translator","text":"","title":"Memory Address Translator"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#c-driver-for-pcie-interfacing","text":"","title":"C driver for PCIe interfacing"},{"location":"Projects/Current/ajit_hpc_blocks/Generic_interface/#software-testbench","text":"","title":"Software Testbench"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/","text":"Introduction AJIT is India's first indigenous processor which has been designed at IIT Bombay and is currently in its second design and manufacturing iteration. AJIT is based on the open SPARC-v8 ISA and is currently a 32-bit processor. Post processor design and testing phase we head out to develop a system around the processor in order to create necessary peripheral interfaces on the VC709 FPGA. The plan is to create an independent system which would have running an Operating system designed specifically for AJIT on it, this process is well known as creating a port of an OS to AJIT. Here in this report we describea general peripheral interface plan and the necessary peripherals required to boot an OS and how one can go about creating them and interfacing them with AJIT. Necessary Goals to achieve The necessary tasks that we need to accomplish in order to run an OS on AJIT are: Fulfilling the processor's need of larger DRAM to boot the OS Providing direct access to the debug interface to the processor in the new interface plan Interface of Ajit's internal Ajit FIFO Bus (AFB) to the generic AXI-Lite A host side driver which provides read/write access and is part of the debug interface And finally an OS ported to AJIT Roadmap of the project till now Tasks Timeline DRAM Interface development ( Jul'18 ) PCIe-AXI Interface development ( Jul'18 - Aug'18 ) HLS synthesis of Debug FIFOs and their controllers ( Aug'18 ) Custom IP packaging of inhouse AFB-AXI Interface ( Aug'18-Sep'18 ) Development of a Processor Link Interface( TIP ) ( Sep'18 ) OS Porting to AJIT( WIP ) ( Sep'18 - Oct'18 ) Current Stage of the project Testing of FPGA model for memory speeds offered which directly affects data access time and more critically boot time. Testing of processor link interface for intermediate boot critical debugging and serial console printing. Setting up of past contributions in the OS port and looking forward in the directions to improve the boot-debug cycle. Collaborators Mr. Datar Mandar, Phd, HPC Lab Abbreviations Used Abbreviation Full Form PCIe Peripheral Component Interconnect Express AFB AJIT FIFO Bus WIP Work in progress TIP Testing in progress","title":"Introduction"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#introduction","text":"AJIT is India's first indigenous processor which has been designed at IIT Bombay and is currently in its second design and manufacturing iteration. AJIT is based on the open SPARC-v8 ISA and is currently a 32-bit processor. Post processor design and testing phase we head out to develop a system around the processor in order to create necessary peripheral interfaces on the VC709 FPGA. The plan is to create an independent system which would have running an Operating system designed specifically for AJIT on it, this process is well known as creating a port of an OS to AJIT. Here in this report we describea general peripheral interface plan and the necessary peripherals required to boot an OS and how one can go about creating them and interfacing them with AJIT.","title":"Introduction"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#necessary-goals-to-achieve","text":"The necessary tasks that we need to accomplish in order to run an OS on AJIT are: Fulfilling the processor's need of larger DRAM to boot the OS Providing direct access to the debug interface to the processor in the new interface plan Interface of Ajit's internal Ajit FIFO Bus (AFB) to the generic AXI-Lite A host side driver which provides read/write access and is part of the debug interface And finally an OS ported to AJIT","title":"Necessary Goals to achieve"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#roadmap-of-the-project-till-now","text":"Tasks Timeline DRAM Interface development ( Jul'18 ) PCIe-AXI Interface development ( Jul'18 - Aug'18 ) HLS synthesis of Debug FIFOs and their controllers ( Aug'18 ) Custom IP packaging of inhouse AFB-AXI Interface ( Aug'18-Sep'18 ) Development of a Processor Link Interface( TIP ) ( Sep'18 ) OS Porting to AJIT( WIP ) ( Sep'18 - Oct'18 )","title":"Roadmap of the project till now"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#current-stage-of-the-project","text":"Testing of FPGA model for memory speeds offered which directly affects data access time and more critically boot time. Testing of processor link interface for intermediate boot critical debugging and serial console printing. Setting up of past contributions in the OS port and looking forward in the directions to improve the boot-debug cycle.","title":"Current Stage of the project"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#collaborators","text":"Mr. Datar Mandar, Phd, HPC Lab","title":"Collaborators"},{"location":"Projects/Current/ajit_hpc_blocks/Introduction/#abbreviations-used","text":"Abbreviation Full Form PCIe Peripheral Component Interconnect Express AFB AJIT FIFO Bus WIP Work in progress TIP Testing in progress","title":"Abbreviations Used"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/","text":"PCIe-AXI Interface Need for a PCIe interface In the previous iteration of OS booting on AJIT processor the OS utilized the block RAM of the processor which was of size 4 MB and the size of the kernel was just 2 MB and now as we increase the RAM size to accomodate a heavy duty OS we need a faster way to write the image to the storage instead of JTAG and we are just in luck since VC709 has a PCIe interface and can be plugged directly into the PCIe slot of the host CPU motherboard. Right now we lack a flash interface in our system which would have allowed us to have a nonvolatile memory block which would not require to be written at every power on-off cycle of the board. Here we utilize the PCIe interface of the VC709 board. Basics of PCIe interface Before diving into the implemenation details we need to revisit some essential basics of PCI express peripherals that would prove to be useful here. On a Linux machine one can type lspci -vv in the console and get a detailed output of the PCIe devices connected to the machine. Its a long list so I will explain with the example of the VC709 board. { Get the screenshot and explain it } { Explain the sys/resource } BARs and address translation Whenever reading or writing we proceed with a method of providing a base address and the offset along with it which would amount to the actual address. We operate here with what is knows as BAR( Base Address Register ) , it is the register which would hold the base/start address of your Memory block which is DRAM here. One can make multiple BARs with different base addresses to access different parts of the memory. Each BAR is set with a base address and an address range which it can access. Each bar created by the user gives rise to a new resource file in the /sys/ directory of the host CPU OS which as explained later in driver development can be used to access the respective memory locations. Our implementation The figure below shows our PCIe-AXI interface implementation as part of a generic AXI peripheral system. Testing of Interface We tested this with a PCI express inerface from the Host CPU using the pcimem utility. Later on we developed a C driver for creating an API interface which provides helper functions for sending and receiving information through the PCIe-AXI interface to the FPGA model.","title":"PCIe-AXI Interface"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#pcie-axi-interface","text":"","title":"PCIe-AXI Interface"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#need-for-a-pcie-interface","text":"In the previous iteration of OS booting on AJIT processor the OS utilized the block RAM of the processor which was of size 4 MB and the size of the kernel was just 2 MB and now as we increase the RAM size to accomodate a heavy duty OS we need a faster way to write the image to the storage instead of JTAG and we are just in luck since VC709 has a PCIe interface and can be plugged directly into the PCIe slot of the host CPU motherboard. Right now we lack a flash interface in our system which would have allowed us to have a nonvolatile memory block which would not require to be written at every power on-off cycle of the board. Here we utilize the PCIe interface of the VC709 board.","title":"Need for a PCIe interface"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#basics-of-pcie-interface","text":"Before diving into the implemenation details we need to revisit some essential basics of PCI express peripherals that would prove to be useful here. On a Linux machine one can type lspci -vv in the console and get a detailed output of the PCIe devices connected to the machine. Its a long list so I will explain with the example of the VC709 board. { Get the screenshot and explain it } { Explain the sys/resource }","title":"Basics of PCIe interface"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#bars-and-address-translation","text":"Whenever reading or writing we proceed with a method of providing a base address and the offset along with it which would amount to the actual address. We operate here with what is knows as BAR( Base Address Register ) , it is the register which would hold the base/start address of your Memory block which is DRAM here. One can make multiple BARs with different base addresses to access different parts of the memory. Each BAR is set with a base address and an address range which it can access. Each bar created by the user gives rise to a new resource file in the /sys/ directory of the host CPU OS which as explained later in driver development can be used to access the respective memory locations.","title":"BARs and address translation"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#our-implementation","text":"The figure below shows our PCIe-AXI interface implementation as part of a generic AXI peripheral system.","title":"Our implementation"},{"location":"Projects/Current/ajit_hpc_blocks/PCI_AXI_Interface/#testing-of-interface","text":"We tested this with a PCI express inerface from the Host CPU using the pcimem utility. Later on we developed a C driver for creating an API interface which provides helper functions for sending and receiving information through the PCIe-AXI interface to the FPGA model.","title":"Testing of Interface"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/","text":"Processor Link Need for the Processor Link For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations. Driver API The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... } Approach to write the driver Driver infrastructure can be broken down in four simple parts: fpga_open(...) As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions. fpga_close(...) As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device. send_u64_to_processor(...) This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns. recv_u64_from_processor(...) This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one. Explaining polling method The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made. Need for interrupt based support in future To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus. Testing of the driver Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"Processor Link"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#processor-link","text":"","title":"Processor Link"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#need-for-the-processor-link","text":"For the previous iteration of already developed software programs for AJIT to work with the new FPGA model we need to keep the same names of the API calls and need to have a Driver API interface which provides the same functions but with non-trivial and different inherent implementations.","title":"Need for the Processor Link"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#driver-api","text":"The four black boxed functions that the API provides are: int fpga_open(...){ ... } int fpga_close(...){ ... } int recv_u64_from_processor(uint64_t* r_word){ ... } int send_u64_to_processor(uint64_t* s_word){ ... }","title":"Driver API"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#approach-to-write-the-driver","text":"Driver infrastructure can be broken down in four simple parts:","title":"Approach to write the driver"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#fpga_open","text":"As would be evident by the name this function opens a fpga device and returns the status of the opened device and prints error message if it fails to open the device. This function takes a string defined by a C preprocessor which points to the resource file of the fpga device. Every resource file created represents a BAR created during the hardware flow. To open the fpga device it makes a open(...) system call with Read, Write and I/O SYNC flags to have the access permissions.","title":"fpga_open(...)"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#fpga_close","text":"As would be evident by the name this function closes an already open fpga device and returns the status of the device and prints error message if it fails to close the device. This function unmaps the memory mapped base address of the FIFO controller and then makes close(...) system call to close the fpga device.","title":"fpga_close(...)"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#send_u64_to_processor","text":"This function is used to send a 64-bit word to the processor core. The approach here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access. We ineffect get a pointer to the memory location of the data registers which we can dereference to write our splitted data one by one. This function after writing the data registers, polls the success bit in the control register and as soon as it is set the function returns.","title":"send_u64_to_processor(...)"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#recv_u64_from_processor","text":"This function is used to receive a 64-bit word from the processor core. The approach again here is to memory map the base address of the FIFO controllers and providing the required offset to the data registers. For memory mapping we use mmap with PROT_READ and PROT_WRITE flags set so that we have the read-write access.This function before reading the data registers, polls the valid bit in the control register and as soon as it is set the function reads the data. We ineffect get a pointer to the memory location of the data registers which we can dereference to read our splitted data one by one.","title":"recv_u64_from_processor(...)"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#explaining-polling-method","text":"The way in we perform a transfer of a 64-bit number is that we split it into two 32-bit numbers and write it out to 32 -bit memory mapped registers inside the FIFO controllers from where the data is sent to the Debug FIFOs.For simplicity the driver currently continuosly polls the control register of the FIFO controllers and checks for the status bit to set and finishes the API call as soon it sets and returns the received data/status dependent upon the API call made.","title":"Explaining polling method"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#need-for-interrupt-based-support-in-future","text":"To avoid the redundant polling approach we are going to introduce an interrupt based driver API which would would contain ISR routines responding to the PCIe-AXI interface generated interrupts. For this functionality we most probably would need to have a Kernel Space Module which would have direct access to Kernel ISR routines and would allow us to respond to specifically VC709 generated interrupts on the PCIe bus.","title":"Need for interrupt based support in future"},{"location":"Projects/Current/ajit_hpc_blocks/Processor_Link/#testing-of-the-driver","text":"Preliminary tests were conducted using pcimem utility with successfull results but extensive tests on the driver are still pending and would be completed in near future.","title":"Testing of the driver"},{"location":"Projects/Current/ajit_hpc_blocks/ajit_core/","text":"AJIT Core","title":"AJIT Core"},{"location":"Projects/Current/ajit_hpc_blocks/ajit_core/#ajit-core","text":"","title":"AJIT Core"},{"location":"Projects/Current/ajit_hpc_blocks/combining_ips/","text":"Combining IPs AJIT Core, AFB-AXI Bridge and the Debug Bridge","title":"Combining IPs"},{"location":"Projects/Current/ajit_hpc_blocks/combining_ips/#combining-ips","text":"","title":"Combining IPs"},{"location":"Projects/Current/ajit_hpc_blocks/combining_ips/#ajit-core-afb-axi-bridge-and-the-debug-bridge","text":"","title":"AJIT Core, AFB-AXI Bridge and the Debug Bridge"},{"location":"Ricing/boot_screen/","text":"","title":"Boot screen changes"},{"location":"Ricing/grub/","text":"","title":"Tinkering with GRUB"},{"location":"Ricing/gtk/","text":"","title":"GTK changes"},{"location":"Tutorials/Aa/","text":"","title":"Aa"},{"location":"Tutorials/Advanced_HLS/","text":"Advanced HLS example","title":"Advanced HLS example"},{"location":"Tutorials/Advanced_HLS/#advanced-hls-example","text":"","title":"Advanced HLS example"},{"location":"Tutorials/Basic_HLS/","text":"Basic HLS example HLS Adder A simple AXI interface AXI block. CPP file #include ap_int.h #include hls_stream.h void Adder(int data_in_1, int data_in_2, int *data_out) { #pragma HLS INTERFACE s_axilite port=data_in_1 #pragma HLS INTERFACE s_axilite port=data_in_2 #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=return *data_out = data_in_1 + data_in_2; }","title":"Basic HLS example"},{"location":"Tutorials/Basic_HLS/#basic-hls-example","text":"","title":"Basic HLS example"},{"location":"Tutorials/Basic_HLS/#hls-adder","text":"A simple AXI interface AXI block.","title":"HLS Adder"},{"location":"Tutorials/Basic_HLS/#cpp-file","text":"#include ap_int.h #include hls_stream.h void Adder(int data_in_1, int data_in_2, int *data_out) { #pragma HLS INTERFACE s_axilite port=data_in_1 #pragma HLS INTERFACE s_axilite port=data_in_2 #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=return *data_out = data_in_1 + data_in_2; }","title":"CPP file"},{"location":"Tutorials/HLS/","text":"High Level Synthesis Tools in Vivado Setup You need a CPP file where you describe your block and a tcl scipt and a Makefile to compile the project. CPP file #include ap_int.h #include hls_stream.h void Name-of-your-block (int data_in_1, int data_in_2, int *data_out) /* Port declarations similar to function arguments */ { /* pragmas to inform compiler about the type of ports */ #pragma HLS INTERFACE s_axilite port=data_in_1 #pragma HLS INTERFACE s_axilite port=data_in_2 #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=return *data_out = /* Some operation on inputs producing an output */ } tcl script open_project test set_top test add_files test.cpp open_solution test set_part {xc7vx690tffg1761-2} create_clock -period 10 -name default csynth_design export_design -format ip_catalog exit Makefile VIVADO_HLS17_1=/home/Xilinx_2017.1/Vivado_HLS/2017.1/bin/vivado_hls default: all all: hls checkpaths: @ls $(VIVADO_HLS17_1) /dev/null 2 1 || echo Error : incorrect path for vivado_hls. @ls $(VIVADO_HLS17_1) /dev/null 2 1 hls: checkpaths $(VIVADO_HLS17_1) -f hls.tcl clean: rm -rf Adder vivado_hls.log Basic Example Intermediate Example Advanced Example","title":"High Level Synthesis Tools in Vivado"},{"location":"Tutorials/HLS/#high-level-synthesis-tools-in-vivado","text":"","title":"High Level Synthesis Tools in Vivado"},{"location":"Tutorials/HLS/#setup","text":"You need a CPP file where you describe your block and a tcl scipt and a Makefile to compile the project.","title":"Setup"},{"location":"Tutorials/HLS/#cpp-file","text":"#include ap_int.h #include hls_stream.h void Name-of-your-block (int data_in_1, int data_in_2, int *data_out) /* Port declarations similar to function arguments */ { /* pragmas to inform compiler about the type of ports */ #pragma HLS INTERFACE s_axilite port=data_in_1 #pragma HLS INTERFACE s_axilite port=data_in_2 #pragma HLS INTERFACE s_axilite port=data_out #pragma HLS INTERFACE s_axilite port=return *data_out = /* Some operation on inputs producing an output */ }","title":"CPP file"},{"location":"Tutorials/HLS/#tcl-script","text":"open_project test set_top test add_files test.cpp open_solution test set_part {xc7vx690tffg1761-2} create_clock -period 10 -name default csynth_design export_design -format ip_catalog exit","title":"tcl script"},{"location":"Tutorials/HLS/#makefile","text":"VIVADO_HLS17_1=/home/Xilinx_2017.1/Vivado_HLS/2017.1/bin/vivado_hls default: all all: hls checkpaths: @ls $(VIVADO_HLS17_1) /dev/null 2 1 || echo Error : incorrect path for vivado_hls. @ls $(VIVADO_HLS17_1) /dev/null 2 1 hls: checkpaths $(VIVADO_HLS17_1) -f hls.tcl clean: rm -rf Adder vivado_hls.log Basic Example Intermediate Example Advanced Example","title":"Makefile"},{"location":"Tutorials/Intermediate_HLS/","text":"Intermediate HLS example","title":"Intermediate HLS example"},{"location":"Tutorials/Intermediate_HLS/#intermediate-hls-example","text":"","title":"Intermediate HLS example"},{"location":"Tutorials/VHDL/","text":"","title":"VHDL"},{"location":"Tutorials/ajit_port_defs/","text":"AJIT FPGA HPC Interface Creation AFB AFB RESP Port Name Port Description Direction VHDL Data Type AFB_RESPONSE_pipe_write_data in std_logic_vector(32 downto 0) AFB_RESPONSE_pipe_write_req in std_logic_vector(0 downto 0) AFB_RESPONSE_pipe_write_ack out std_logic_vector(0 downto 0) AFB REQ Port Name Port Description Direction VHDL Data Type AFB_COMMAND_pipe_read_data out std_logic_vector(73 downto 0) AFB_COMMAND_pipe_read_req in std_logic_vector(0 downto 0) AFB_COMMAND_pipe_read_ack out std_logic_vector(0 downto 0) Debug Bridge FIFO TO AJIT Port Name Port Description Direction VHDL Data Type RIFFA_to_CPU_pipe_write_data in std_logic_vector(63 downto 0) RIFFA_to_CPU_pipe_write_req in std_logic_vector(0 downto 0) RIFFA_to_CPU_pipe_write_ack out std_logic_vector(0 downto 0) AJIT TO FIFO Port Name Port Description Direction VHDL Data Type CPU_to_RIFFA_pipe_read_data out std_logic_vector(63 downto 0) CPU_to_RIFFA_pipe_read_req in std_logic_vector(0 downto 0) CPU_to_RIFFA_pipe_read_ack out std_logic_vector(0 downto 0) Extra Ports Port Name Port Description Direction VHDL Data Type EXTERNAL_INTERRUPT in std_logic_vector(0 downto 0) clk in std_logic reset in std_logic","title":"AJIT FPGA HPC Interface Creation"},{"location":"Tutorials/ajit_port_defs/#ajit-fpga-hpc-interface-creation","text":"","title":"AJIT FPGA HPC Interface Creation"},{"location":"Tutorials/ajit_port_defs/#afb","text":"","title":"AFB"},{"location":"Tutorials/ajit_port_defs/#afb-resp","text":"Port Name Port Description Direction VHDL Data Type AFB_RESPONSE_pipe_write_data in std_logic_vector(32 downto 0) AFB_RESPONSE_pipe_write_req in std_logic_vector(0 downto 0) AFB_RESPONSE_pipe_write_ack out std_logic_vector(0 downto 0)","title":"AFB RESP"},{"location":"Tutorials/ajit_port_defs/#afb-req","text":"Port Name Port Description Direction VHDL Data Type AFB_COMMAND_pipe_read_data out std_logic_vector(73 downto 0) AFB_COMMAND_pipe_read_req in std_logic_vector(0 downto 0) AFB_COMMAND_pipe_read_ack out std_logic_vector(0 downto 0)","title":"AFB REQ"},{"location":"Tutorials/ajit_port_defs/#debug-bridge","text":"","title":"Debug Bridge"},{"location":"Tutorials/ajit_port_defs/#fifo-to-ajit","text":"Port Name Port Description Direction VHDL Data Type RIFFA_to_CPU_pipe_write_data in std_logic_vector(63 downto 0) RIFFA_to_CPU_pipe_write_req in std_logic_vector(0 downto 0) RIFFA_to_CPU_pipe_write_ack out std_logic_vector(0 downto 0)","title":"FIFO TO AJIT"},{"location":"Tutorials/ajit_port_defs/#ajit-to-fifo","text":"Port Name Port Description Direction VHDL Data Type CPU_to_RIFFA_pipe_read_data out std_logic_vector(63 downto 0) CPU_to_RIFFA_pipe_read_req in std_logic_vector(0 downto 0) CPU_to_RIFFA_pipe_read_ack out std_logic_vector(0 downto 0)","title":"AJIT TO FIFO"},{"location":"Tutorials/ajit_port_defs/#extra-ports","text":"Port Name Port Description Direction VHDL Data Type EXTERNAL_INTERRUPT in std_logic_vector(0 downto 0) clk in std_logic reset in std_logic","title":"Extra Ports"},{"location":"Tutorials/build_tools/","text":"Build Systems Make tools Auto tools Useful Link CMake Python Based Meson build system","title":"Build Systems"},{"location":"Tutorials/build_tools/#build-systems","text":"","title":"Build Systems"},{"location":"Tutorials/build_tools/#make-tools","text":"","title":"Make tools"},{"location":"Tutorials/build_tools/#auto-tools","text":"Useful Link","title":"Auto tools"},{"location":"Tutorials/build_tools/#cmake","text":"","title":"CMake"},{"location":"Tutorials/build_tools/#python-based","text":"","title":"Python Based"},{"location":"Tutorials/build_tools/#meson-build-system","text":"","title":"Meson build system"},{"location":"Tutorials/c_and_tcl/","text":"Using C with Tcl","title":"Using C with Tcl"},{"location":"Tutorials/c_and_tcl/#using-c-with-tcl","text":"","title":"Using C with Tcl"},{"location":"Tutorials/create_interface/","text":"Creating interface definitions in Vivado 2017.1 Introduction Port Definition Table An example table:- Port Name Port Description Direction VHDL Data Type data in std_logic_vector(32 downto 0) req in std_logic_vector(0 downto 0) ack out std_logic_vector(0 downto 0) Interface creation Go to Tools Create Interface Definition AJIT Processor Example","title":"Creating interface definitions in Vivado 2017.1"},{"location":"Tutorials/create_interface/#creating-interface-definitions-in-vivado-20171","text":"","title":"Creating interface definitions in Vivado 2017.1"},{"location":"Tutorials/create_interface/#introduction","text":"","title":"Introduction"},{"location":"Tutorials/create_interface/#port-definition-table","text":"An example table:- Port Name Port Description Direction VHDL Data Type data in std_logic_vector(32 downto 0) req in std_logic_vector(0 downto 0) ack out std_logic_vector(0 downto 0)","title":"Port Definition Table"},{"location":"Tutorials/create_interface/#interface-creation","text":"Go to Tools Create Interface Definition","title":"Interface creation"},{"location":"Tutorials/create_interface/#ajit-processor-example","text":"","title":"AJIT Processor Example"},{"location":"Tutorials/intro/","text":"Tutorials Embedding Tcl in C Here we discuss how to utilize the capabilities of tcl commands when intermixed with C. Vivado 2017.1 Create Interface Definition Here we discuss how to create custom interface definitions in Vivado 2017.1. Vivado 2017.1 HLS Tutorials Here I first present a beginner level tutorial of Vivado HLS, where we will build a simple Adder HDL block through basically what would be a C++ program. And from there on we move to more advanced examples of designing Stream FIFOs and their AXI controllers. Then we move on to designing a Memory Address Translator Block which can read and write the memory through address translation. Aa HLS Here we discuss some basic VHDL blocks generated through Aa HLS ( known as AHIR ) and how to simulate them in software and hardware. I will also discuss some of the semantics of Aa here. VHDL Examples Here we discuss some basic VHDL examples and how to simulate them in gtkwave. I will also discuss some of the useful semantics of VHDL here. Introduction to Build Systems Here we discuss some build systems and how to setup a basic project for each of them. We will also discuss some advantages of each over other build alternatives.","title":"Tutorials"},{"location":"Tutorials/intro/#tutorials","text":"","title":"Tutorials"},{"location":"Tutorials/intro/#embedding-tcl-in-c","text":"Here we discuss how to utilize the capabilities of tcl commands when intermixed with C.","title":"Embedding Tcl in C"},{"location":"Tutorials/intro/#vivado-20171-create-interface-definition","text":"Here we discuss how to create custom interface definitions in Vivado 2017.1.","title":"Vivado 2017.1 Create Interface Definition"},{"location":"Tutorials/intro/#vivado-20171-hls-tutorials","text":"Here I first present a beginner level tutorial of Vivado HLS, where we will build a simple Adder HDL block through basically what would be a C++ program. And from there on we move to more advanced examples of designing Stream FIFOs and their AXI controllers. Then we move on to designing a Memory Address Translator Block which can read and write the memory through address translation.","title":"Vivado 2017.1 HLS Tutorials"},{"location":"Tutorials/intro/#aa-hls","text":"Here we discuss some basic VHDL blocks generated through Aa HLS ( known as AHIR ) and how to simulate them in software and hardware. I will also discuss some of the semantics of Aa here.","title":"Aa HLS"},{"location":"Tutorials/intro/#vhdl-examples","text":"Here we discuss some basic VHDL examples and how to simulate them in gtkwave. I will also discuss some of the useful semantics of VHDL here.","title":"VHDL Examples"},{"location":"Tutorials/intro/#introduction-to-build-systems","text":"Here we discuss some build systems and how to setup a basic project for each of them. We will also discuss some advantages of each over other build alternatives.","title":"Introduction to Build Systems"}]}